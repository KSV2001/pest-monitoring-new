{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9efa0169",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import json\n",
    "import os\n",
    "from os.path import join\n",
    "from PIL import Image\n",
    "from pycocotools.coco import COCO\n",
    "import torchvision\n",
    "from src.data.COCO.transforms import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "285396c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class COCODataset(Dataset):\n",
    "    \"\"\"\n",
    "    A PyTorch Dataset class to be used in a PyTorch DataLoader to create batches.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root, split = 'train', version = '2014', transforms=None):\n",
    "        \"\"\"\n",
    "        :param image_dir: folder where data files are stored\n",
    "        :param annotations_dir: folder where annotation files corresponding to splits are stored\n",
    "        :param split: \n",
    "        :param split: split, one of 'train' or 'val' or 'test'\n",
    "        \"\"\"\n",
    "        assert split in {'train', 'val', 'test'}\n",
    "        self.root = root\n",
    "        self.verion = version\n",
    "        self.split = split\n",
    "        \n",
    "        # Load coco object using pycoco\n",
    "        # Load coco object using pycoco\n",
    "        if split == 'test':\n",
    "            self.coco = COCO(join(root, 'annotations', f'image_info_test{version}.json'))\n",
    "        else:\n",
    "            self.coco = COCO(join(root, 'annotations', f'instances_{split}{version}.json'))\n",
    "        self.image_dir = join(root, f'{split}')\n",
    "        self.image_ids = list(sorted(self.coco.imgs.keys()))\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Own coco file\n",
    "        coco = self.coco\n",
    "        \n",
    "        # Image ID\n",
    "        img_id = self.image_ids[index]\n",
    "        # List: get annotation id from coco\n",
    "        ann_ids = coco.getAnnIds(imgIds=img_id)\n",
    "        # Dictionary: target coco_annotation file for an image\n",
    "        coco_annotation = coco.loadAnns(ann_ids)\n",
    "        # path for input image\n",
    "        path = coco.loadImgs(img_id)[0][\"file_name\"]\n",
    "        # open the input image\n",
    "        img = Image.open(os.path.join(self.image_dir, path))\n",
    "        # number of objects in the image\n",
    "        num_objs = len(coco_annotation)\n",
    "\n",
    "        # Bounding boxes for objects\n",
    "        # In coco format, bbox = [xmin, ymin, width, height]\n",
    "        # In pytorch, the input should be [xmin, ymin, xmax, ymax]\n",
    "        boxes = []\n",
    "        for i in range(num_objs):\n",
    "            xmin = coco_annotation[i][\"bbox\"][0]\n",
    "            ymin = coco_annotation[i][\"bbox\"][1]\n",
    "            xmax = xmin + coco_annotation[i][\"bbox\"][2]\n",
    "            ymax = ymin + coco_annotation[i][\"bbox\"][3]\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        # Labels (In my case, I only one class: target class or background)\n",
    "        labels = torch.ones((num_objs,), dtype=torch.int64)\n",
    "        # Tensorise img_id\n",
    "        img_id = torch.tensor([img_id])\n",
    "        # Size of bbox (Rectangular)\n",
    "        areas = []\n",
    "        for i in range(num_objs):\n",
    "            areas.append(coco_annotation[i][\"area\"])\n",
    "        areas = torch.as_tensor(areas, dtype=torch.float32)\n",
    "        # Iscrowd\n",
    "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "\n",
    "        # Annotation is in dictionary format\n",
    "        my_annotation = {}\n",
    "        my_annotation[\"boxes\"] = boxes\n",
    "        my_annotation[\"labels\"] = labels\n",
    "        my_annotation[\"image_id\"] = img_id\n",
    "        my_annotation[\"area\"] = areas\n",
    "        my_annotation[\"iscrowd\"] = iscrowd\n",
    "        \n",
    "#         import ipdb; ipdb.set_trace()\n",
    "        if self.transforms is not None:\n",
    "            data = {\n",
    "                \"image\": np.array(img),\n",
    "                \"bboxes\": my_annotation[\"boxes\"].numpy(),\n",
    "                \"labels\": my_annotation[\"labels\"].numpy(),\n",
    "            }\n",
    "            augmented = self.transforms(**data)\n",
    "            img = torch.tensor(augmented['image'])\n",
    "            bboxes = torch.tensor(augmented['bboxes'])\n",
    "            labels = torch.tensor(augmented['labels'])\n",
    "\n",
    "        return img, bboxes, labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        \"\"\"\n",
    "        Since each image may have a different number of objects, we need a collate function (to be passed to the DataLoader).\n",
    "        This describes how to combine these tensors of different sizes. We use lists.\n",
    "        Note: this need not be defined in this Class, can be standalone.\n",
    "        :param batch: an iterable of N sets from __getitem__()\n",
    "        :return: a tensor of images, lists of varying-size tensors of bounding boxes, labels, and difficulties\n",
    "        \"\"\"\n",
    "\n",
    "        images = list()\n",
    "        boxes = list()\n",
    "        labels = list()\n",
    "\n",
    "        for b in batch:\n",
    "            images.append(b[0])\n",
    "            boxes.append(b[1])\n",
    "            labels.append(b[2])\n",
    "\n",
    "        images = torch.stack(images, dim=0)\n",
    "\n",
    "        return images, boxes, labels  # tensor (N, 3, 300, 300), 3 lists of N tensors each        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8836d2af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=7.49s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "dataset = COCODataset(root = '/data/coco2014/raw/',\n",
    "                      split = 'val', version = '2014',\n",
    "                     transforms = get_transform())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "115bd646",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:84: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[173, 174, 177,  ...,  67,  74,  70],\n",
       "          [176, 177, 179,  ...,  73,  75,  69],\n",
       "          [177, 178, 178,  ...,  73,  73,  68],\n",
       "          ...,\n",
       "          [167, 172, 172,  ..., 190, 192, 194],\n",
       "          [184, 181, 194,  ..., 190, 187, 186],\n",
       "          [187, 187, 187,  ..., 141, 138, 145]],\n",
       " \n",
       "         [[142, 146, 148,  ...,  73,  74,  76],\n",
       "          [145, 148, 149,  ...,  74,  76,  76],\n",
       "          [147, 148, 148,  ...,  73,  76,  76],\n",
       "          ...,\n",
       "          [109, 113, 116,  ..., 175, 181, 184],\n",
       "          [184, 185, 206,  ..., 149, 157, 156],\n",
       "          [184, 184, 182,  ...,  68,  69,  72]],\n",
       " \n",
       "         [[ 77,  77,  78,  ...,  40,  42,  40],\n",
       "          [ 80,  80,  79,  ...,  40,  39,  40],\n",
       "          [ 80,  78,  78,  ...,  38,  41,  41],\n",
       "          ...,\n",
       "          [ 67,  67,  75,  ..., 171, 175, 179],\n",
       "          [151, 146, 184,  ..., 149, 154, 153],\n",
       "          [151, 153, 153,  ...,  71,  72,  77]]], dtype=torch.uint8),\n",
       " tensor([[236.9800, 142.5100, 261.6800, 212.0100],\n",
       "         [  7.0300, 167.7600, 156.3500, 262.6300],\n",
       "         [557.2100, 209.1900, 638.5600, 287.9200],\n",
       "         [358.9800, 218.0500, 414.9800, 320.8800],\n",
       "         [290.6900, 218.0000, 352.5200, 316.4800],\n",
       "         [413.2000, 223.0100, 443.3700, 304.3700],\n",
       "         [317.4000, 219.2400, 338.9800, 230.8300],\n",
       "         [412.8000, 157.6100, 465.8500, 295.6200],\n",
       "         [384.4300, 172.2100, 399.5500, 207.9500],\n",
       "         [512.2200, 205.7500, 526.9600, 221.7200],\n",
       "         [493.1000, 174.3400, 513.3900, 282.6500],\n",
       "         [604.7700, 305.8900, 619.1100, 351.6000],\n",
       "         [613.2400, 308.2400, 626.1200, 354.6800],\n",
       "         [447.7700, 121.1200, 461.7400, 143.0000],\n",
       "         [549.0600, 309.4300, 585.7400, 399.1000],\n",
       "         [350.7600, 208.8400, 362.1300, 231.3900],\n",
       "         [412.2500, 219.0200, 421.8800, 231.5400],\n",
       "         [241.2400, 194.9900, 255.4600, 212.6200],\n",
       "         [336.7900, 199.5000, 346.5200, 216.2300],\n",
       "         [321.2100, 231.2200, 446.7700, 320.1500]]),\n",
       " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9294360",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(image, bboxes, category_ids, category_id_to_name):\n",
    "    img = image.copy()\n",
    "    for bbox, category_id in zip(bboxes, category_ids):\n",
    "        class_name = category_id_to_name[category_id]\n",
    "        img = visualize_bbox(img, bbox, class_name)\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "095adad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In my case, just added ToTensor\n",
    "def get_transform():\n",
    "    custom_transforms = []\n",
    "    custom_transforms.append(torchvision.transforms.ToTensor())\n",
    "    return torchvision.transforms.Compose(custom_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3ff61c8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "    ToTensor()\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81be32d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
